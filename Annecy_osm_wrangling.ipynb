{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 2: Wrangle OpenStreetMap Data\n",
    "\n",
    " ## Project : cleaning and sql queries of Annecy open street map data\n",
    "\n",
    "![https://github.com/MariannePERAUD/Udacity_Project_2_Data_wrangling_with_sql/blob/master/Annecy.jpg](Annecy.jpg)\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#Intro\">Introduction</a></li>\n",
    "<li><a href=\"#Set-up\">Initial Set-up</a></li>   \n",
    "<li><a href=\"#assess\">Assess data</a></li>\n",
    "<li><a href=\"#identify\">Identify problems and clean data</a></li>\n",
    "<li><a href=\"#transfercsv\">Write nodes and ways as csv files</a></li>\n",
    "<li><a href=\"#createsql\">Create sql databasis</a></li>\n",
    "<li><a href=\"#checkdb\">Some checks on databasis accuracy and completeness prior to inquiries</a></li>\n",
    "<li><a href=\"#querydb\">Sql queries</a></li>\n",
    "<li><a href=\"#references\">Some links to references used</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Intro'></a>\n",
    "## Introduction\n",
    "\n",
    "Area explored is that of Annecy, in the French Alps, a place that I like and where part of my family leaves.\n",
    "Annecy is near French Alps, not far from Switzerland.\n",
    "\n",
    "\n",
    "\n",
    " minlat=\"45.7996000\" minlon=\"5.9336000\" maxlat=\"45.9323000\" maxlon=\"6.2529000\"\n",
    "\n",
    "I will first check file size and content ; \n",
    "\n",
    "Then identify problems and errors and begin data cleaning to make further analysis on a cleaned dataset;  \n",
    "\n",
    "Transform osm data in csv.files ;  \n",
    "\n",
    "From csv.files, create a databasis in sql with appropriate structure ;\n",
    "\n",
    "Analyse databasis with sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Set-up'></a>\n",
    "## Set-up\n",
    "Import necessary python modules for the analysis and define path to dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8import xml.etree.cElementTree as ET \n",
    "import xml.etree.cElementTree as ET \n",
    "import codecs ##to write unicode files\n",
    "import pprint ## to print easier to read dictionnaries\n",
    "import csv ## to read and write csv file\n",
    "import os ## to get file size \n",
    "import pandas as pd ## I am more familiar with dataframe use than plan dictionnaries\n",
    "import re ## to search characters in dataset\n",
    "import sqlite3 ## to use SQL databasis\n",
    "import seaborn ## to trace distribution plot for users\n",
    "from collections import defaultdict ## to avoid Keyerrors. Will return default value for missing values. \n",
    "                                    ##As advised by Udacity:-)\n",
    "DATASET = \"Annecy.osm\"\n",
    "\n",
    "pd.set_option('display.width', 2000) ## in order to get nicer display of pandas dataframes on screen\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "   ## Assess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check that size of the file is greater than 50Mb#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=os.path.getsize(DATASET)/1000000\n",
    "size=round(size)\n",
    "##Map's size and references\n",
    "##Source: https://prograide.com/pregunta/6464/obtenir-la-taille-du-fichier-en-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Count number of different tags in the file#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags={}\n",
    "def count_tags(filename):\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    \n",
    "    return tags\n",
    "\n",
    "tags = count_tags(DATASET)\n",
    "\n",
    "  \n",
    "    ## see summary at the end of paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Count Number of users as of August 28th 2020 (extraction date)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    \"\"\"\n",
    "    Count the user id in the filename.\n",
    "    \"\"\"\n",
    "    users = set()\n",
    "    for Marianne, element in ET.iterparse(filename):\n",
    "        try:\n",
    "            users.add(element.attrib['uid'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return users\n",
    "\n",
    "users = process_map(DATASET)\n",
    "\n",
    "    ##see summary at the end of paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Statistics summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Annecy.osm file is 152 Mb, so higher than 50Mb requested\n",
      "\n",
      "Number of tags per type\n",
      "               0\n",
      "osm       1     \n",
      "note      1     \n",
      "meta      1     \n",
      "bounds    1     \n",
      "node      614821\n",
      "tag       322728\n",
      "way       91637 \n",
      "nd        807410\n",
      "relation  1049  \n",
      "member    45585 \n",
      "\n",
      "926 users until August 28th 2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Size of Annecy.osm file is\",size,\"Mb, so higher than 50Mb requested\")\n",
    "print(\"\")\n",
    "print(\"Number of tags per type\")\n",
    "print(pd.DataFrame([tags]).T)\n",
    "print(\"\")\n",
    "print(len(users),\"users until August 28th 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='identify'></a>\n",
    "   ## Identify and clean problems\n",
    "\n",
    "Analyses focuses on identifying problems in street names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess various types of street names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French street names are beginning with streetname type,  \n",
    "Rue Lafayette or Allées Wilson etc...  \n",
    "In another node, you can find housenumber  \n",
    "Decoding of Annecy street names has to be a little different from that of US street names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FICHAUDIT = \"Annecy.osm\"\n",
    "\n",
    "##identifies name of the street in French addresses \n",
    "\n",
    "street_type_re = re.compile(r\"\"\"\n",
    "^                                    ## for the beginning of the string\n",
    "\\b                                   ## might be empty, but only at the beginning\n",
    "\\S+                                  ## then followed by any character that is not a space\n",
    "\\.?\"\"\",                              \n",
    "re.IGNORECASE|re.VERBOSE) \n",
    "\n",
    "\n",
    "\n",
    "expected = [\"Routes\",\"Maison\",\"ZA\",\"ZI\",\"Palais\",\"Parc\",\"Angon\",\"Escaliers\",\"Le\",\"Les\",\"Lieudit\",\"Chemin\",\"Esplanade\",\"Faubourg\",\"Passage\",\"Pont\",\"Port\",\"Rue\", \"Boulevard\",\"Place\",\"Allée\",\"Avenue\",\"Impasse\",\"Col\",\"Côte\",\"Quai\",\"Rampe\",\"Route\",\"Promenade\",\"Square\",\"Voie\"]\n",
    "##when iterating on the query, we will see that there are many valid possible entries other than rue (=street)\n",
    "\n",
    "##treet_type_re = re.compile(r'^\\b\\S+\\.?', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested functions in order to audit open street map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## create street types set(ie strings at the begining of tags before a blanc)\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "           \n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "## define condition : is in street types\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "## parse all tags for node and way and list all strings corresponding to street types in osm file\n",
    "def audit(osmfile):\n",
    "    osm_file = open(DATASET, \"r\",encoding='utf-8')\n",
    "    street_types = defaultdict(set)\n",
    "    ##print (street_types)\n",
    "    i=0\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        \n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "           \n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'13': {'13 Rue JEAN MOULIN'},\n",
      " 'Georges': {'Georges Paccard'},\n",
      " 'allée': {'allée des Fournais', 'allée des gentianes'},\n",
      " 'avenue': {\"avenue d'Aix Les Bains\",\n",
      "            \"avenue d'Aix les Bains\",\n",
      "            'avenue de France',\n",
      "            'avenue de johnathan',\n",
      "            'avenue de la Mavéria',\n",
      "            'avenue des trois fontaines'},\n",
      " 'chemin': {'chemin de Bellevue', 'chemin des Glaisiers', 'chemin de bellevue'},\n",
      " 'fg': {'fg des balmettes'},\n",
      " 'route': {\"route d'Albertville\",\n",
      "           \"route d'Annecy\",\n",
      "           'route de Brassilly',\n",
      "           'route de Closon',\n",
      "           'route de menthon',\n",
      "           'route des Creuses',\n",
      "           'route du périmètre'},\n",
      " 'rue': {'rue Cassiopée',\n",
      "         'rue Henry Bordeaux',\n",
      "         'rue Royale',\n",
      "         'rue Sommeiller',\n",
      "         'rue de la Gare',\n",
      "         'rue de la Garr',\n",
      "         'rue de la gare',\n",
      "         'rue de la préfecture',\n",
      "         'rue des Glières',\n",
      "         'rue des écoles',\n",
      "         'rue du Pré Faucon',\n",
      "         'rue du Président Favre',\n",
      "         'rue du Travail',\n",
      "         'rue du Vieux Moulin'}}\n"
     ]
    }
   ],
   "source": [
    "##run funtion audit on osm file and print \"unexpected street type dictionnary\"\n",
    "\n",
    "\n",
    "st_types = audit(DATASET)\n",
    "\n",
    "pprint.pprint(dict(st_types))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New mapping dictionary 13 and Georges are not changed at this stage  \n",
    "                         13 should go in another tag (housenumber) and name of the street type should be added to  \n",
    "                         street_nametag at a later stage; \n",
    "Lower case is replaced and abbreviation fg is replaced by Faubourg.\n",
    "\n",
    "If users are not paying attention to street names, can we consider dataset as reliable ?\n",
    "It depends on what we are looking for.\n",
    "Where I leave, I use open street map for recommended itineraries by bicycle, so street names are not so important for me.\n",
    "What is more important is what I can find on my way (shops, parks...).\n",
    "\n",
    "However, let's replace wrong entries in the file that will be imported for queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace wrong upercase/lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mapping = { \"13\": \"13\",\n",
    "           \"Georges\": \"Georges\",\n",
    "            \"allée.\": \"Allée\",\n",
    "            \"chemin\": \"Chemin\",\n",
    "            \"fg\": \"Faubourg\",\n",
    "            \"route\": \"Route\",\n",
    "          \"rue\": \"Rue\"}\n",
    "\n",
    "#update name creates a new dictionnary with \"before and after\"street type entries\n",
    "def update_name(name):\n",
    "    street_type= name.split(' ',1)[0]\n",
    "    street_name= name.split(' ',1)[-1]        \n",
    "\n",
    "    if street_type in mapping:\n",
    "        name = mapping[street_type] + ' ' + street_name  \n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run update creates a new dictionnary where street type entries are corrected when necessary\n",
    "def run_updates(filename):\n",
    "    \n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name)\n",
    "            if better_name != name:\n",
    "                corrected_names[name] = better_name\n",
    "    return corrected_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**QUICK CHECK OF MODIFICATIONS PROPOSAL**                                              0\n",
      "rue Sommeiller          Rue Sommeiller        \n",
      "rue de la gare          Rue de la gare        \n",
      "rue de la Gare          Rue de la Gare        \n",
      "rue Cassiopée           Rue Cassiopée         \n",
      "rue des Glières         Rue des Glières       \n",
      "rue du Président Favre  Rue du Président Favre\n",
      "rue du Vieux Moulin     Rue du Vieux Moulin   \n",
      "rue des écoles          Rue des écoles        \n",
      "rue du Pré Faucon       Rue du Pré Faucon     \n",
      "rue Royale              Rue Royale            \n",
      "rue Henry Bordeaux      Rue Henry Bordeaux    \n",
      "rue de la préfecture    Rue de la préfecture  \n",
      "rue de la Garr          Rue de la Garr        \n",
      "rue du Travail          Rue du Travail        \n",
      "route d'Albertville     Route d'Albertville   \n",
      "route du périmètre      Route du périmètre    \n",
      "route de Closon         Route de Closon       \n",
      "route des Creuses       Route des Creuses     \n",
      "route de Brassilly      Route de Brassilly    \n",
      "route de menthon        Route de menthon      \n",
      "route d'Annecy          Route d'Annecy        \n",
      "fg des balmettes        Faubourg des balmettes\n",
      "chemin de Bellevue      Chemin de Bellevue    \n",
      "chemin des Glaisiers    Chemin des Glaisiers  \n",
      "chemin de bellevue      Chemin de bellevue    \n"
     ]
    }
   ],
   "source": [
    "#corrected names creation for osm file\n",
    "corrected_names = {}   \n",
    "corrected_names = run_updates(DATASET)\n",
    "check2=(pd.DataFrame([corrected_names]).T) \n",
    "print(\"**QUICK CHECK OF MODIFICATIONS PROPOSAL**\",check2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transfercsv'></a>\n",
    "   ## Write nodes and ways as csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File is now ready for import in a csv file and then in SQL databasis.\n",
    "We will create five csv files preparing five SQL tables creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# Prepare modification of dictionnary to import with modified street names\n",
    "def correct_element(v):\n",
    "    if v in corrected_names:\n",
    "        correct_value = corrected_names[v]\n",
    "    else:\n",
    "        correct_value = v\n",
    "    return correct_value\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                   default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "        \n",
    "        for node in element:\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if ':' in node.attrib['k']:\n",
    "                tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = node.attrib['k']\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "        n = 0\n",
    "        for node in element:\n",
    "            if node.tag == 'nd':\n",
    "                way_dict = {}\n",
    "                way_dict['id'] = element.attrib['id']\n",
    "                way_dict['node_id'] = node.attrib['ref']\n",
    "                way_dict['position'] = n\n",
    "                n += 1\n",
    "                way_nodes.append(way_dict)\n",
    "            if node.tag == 'tag':\n",
    "                tag_dict = {}\n",
    "                tag_dict['id'] = element.attrib['id']\n",
    "                if ':' in node.attrib['k']:\n",
    "                    tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                    tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                else:\n",
    "                    tag_dict['type'] = 'regular'\n",
    "                    tag_dict['key'] = node.attrib['k']\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                tags.append(tag_dict)\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    ## create csv files\n",
    "    with codecs.open(NODES_PATH, 'w',encoding='utf-8') as nodes_file, \\\n",
    "    codecs.open(NODE_TAGS_PATH, 'w',encoding='utf-8') as nodes_tags_file, \\\n",
    "    codecs.open(WAYS_PATH, 'w',encoding='utf-8') as ways_file, \\\n",
    "    codecs.open(WAY_NODES_PATH, 'w',encoding='utf-8') as way_nodes_file, \\\n",
    "    codecs.open(WAY_TAGS_PATH, 'w',encoding='utf-8') as way_tags_file:\n",
    "\n",
    "        nodes_writer = csv.DictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = csv.DictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = csv.DictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = csv.DictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = csv.DictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "    ## create headers in csv files\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        \n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if element.tag == 'node':\n",
    "                nodes_writer.writerow(el['node'])\n",
    "                node_tags_writer.writerows(el['node_tags'])\n",
    "            elif element.tag == 'way':\n",
    "                ways_writer.writerow((el['way']))\n",
    "                way_nodes_writer.writerows(el['way_nodes'])\n",
    "                way_tags_writer.writerows(el['way_tags'])\n",
    "                    \n",
    "process_map(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='createsql'></a>\n",
    "   ## Create sql databasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that .csv are created, create tables that will shelter sql databasis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating database on disk\n",
    "sqlite_file = 'Annecy.db'\n",
    "conn = sqlite3.connect('Annecy.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "c.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create sql tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_NODES = \"\"\"\n",
    "\n",
    "\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_NODES_TAGS = \"\"\"\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS = \"\"\"\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_TAGS = \"\"\"\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_NODES = \"\"\"\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "c.execute(QUERY_NODES)\n",
    "c.execute(QUERY_NODES_TAGS)\n",
    "c.execute(QUERY_WAYS)\n",
    "c.execute(QUERY_WAYS_TAGS)\n",
    "c.execute(QUERY_WAYS_NODES)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db1 = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('nodes_tags.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db2 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db3 = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('ways_tags.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db4 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways_nodes.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db5 = [(i['id'], i['node_id'], i['position']) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db1)\n",
    "c.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db2)\n",
    "c.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db3)\n",
    "c.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db4)\n",
    "c.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db5)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='checkdb'></a>\n",
    "   ## Some checks on databasis accuracy and completeness prior to inquiries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several preliminary checks on SQL databasis\n",
    "- Check that databasis is complete (same number of nodes and ways as computed before)\n",
    "- Check that some wrong street names have been corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(614821,)]\n",
      "[(91637,)]\n"
     ]
    }
   ],
   "source": [
    "c.execute('SELECT COUNT(*) FROM nodes')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)\n",
    "\n",
    "c.execute('SELECT COUNT(*) FROM ways')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Chemin de Bellevue',), ('Rue Cassiopée',), ('Rue Cassiopée',)]\n"
     ]
    }
   ],
   "source": [
    "CHECK_CHANGES2= \"\"\"\n",
    "SELECT value \n",
    "FROM ways_tags\n",
    "WHERE value ='Chemin de Bellevue'\n",
    "OR value ='Rue Cassiopée'\n",
    "OR value ='rue Cassiopée'\n",
    "OR value ='Chemin de Bellevue'\n",
    "limit 10;\n",
    "\"\"\"\n",
    "c.execute(CHECK_CHANGES2)\n",
    "all_rows=c.fetchall()\n",
    "print(all_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rue Cassiopée',), ('Rue Cassiopée',)]\n"
     ]
    }
   ],
   "source": [
    "CHECK_CHANGES3= \"\"\"\n",
    "SELECT value \n",
    "FROM nodes_tags\n",
    "WHERE value ='Chemin de Bellevue'\n",
    "OR value ='Rue Cassiopée'\n",
    "OR value ='rue Cassiopée'\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "c.execute(CHECK_CHANGES3)\n",
    "all_rows=c.fetchall()\n",
    "print(all_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='querydb'></a>\n",
    "   ## SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Enquiries about content of the database in SQL, at least..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count number of most active users**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   users, number of contributions\n",
      "0   (Emmanuel Pacaud, 310370)    \n",
      "1   (JFK73, 31147)               \n",
      "2   (Virgile1994, 26631)         \n",
      "3   (botdidier2020, 20812)       \n",
      "4   (chimel38, 20042)            \n",
      "5   (InfosReseaux, 19926)        \n",
      "6   (Zedh, 15058)                \n",
      "7   (EtienneChoveBot, 12144)     \n",
      "8   (CLCF06, 10794)              \n",
      "9   (JFKimports, 10429)          \n",
      "10  (cquest, 9657)               \n",
      "11  (bagnolais, 7842)            \n",
      "12  (rando67, 5219)              \n",
      "13  (Fabsss, 5132)               \n",
      "14  (pyrog, 4894)                \n"
     ]
    }
   ],
   "source": [
    "# Query to show the nicknames *user* and contributions of the top 15 contributors\n",
    "QUERY = '''\n",
    "SELECT DISTINCT nodes.user as USER, COUNT(*) as contribution_number\n",
    "FROM nodes\n",
    "GROUP BY nodes.user\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 15;\n",
    "'''\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "contributors=pd.DataFrame([all_rows]).T\n",
    "contributors.columns=([\"users, number of contributions\"])\n",
    "print(contributors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many contributors and their distribution in % of contributions**   \n",
    "We can see that Emmanuel Pacaud is the most active contributor, with a little more than 50% of contributions. \n",
    "JFK 73 is second contributor with ten time less contributions however 31147 entries.\n",
    "We also see that total of distinct contributors is 793 (to be compared with 926 in the initial .osm file). This is however not surprising because we did not imprt all elements in sql (relations not included)\n",
    "I would have liked to draw distribution but I cannot spit information in a single list or column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "                                        0\n",
      "0    (Emmanuel Pacaud, 50.48135961523761)\n",
      "1    (JFK73, 5.066027347797164)          \n",
      "2    (Virgile1994, 4.3315046167909035)   \n",
      "3    (botdidier2020, 3.3850502829278764) \n",
      "4    (chimel38, 3.2598105790140544)      \n",
      "..                              ...      \n",
      "793  (Adamant1, 0.00016264896612184685)  \n",
      "794  (@rth, 0.00016264896612184685)      \n",
      "795  (74pjb74, 0.00016264896612184685)   \n",
      "796  (5bird, 0.00016264896612184685)     \n",
      "797  (4b696d, 0.00016264896612184685)    \n",
      "\n",
      "[798 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Query to show and contributions allcontributors in %\n",
    "QUERY = '''\n",
    "SELECT DISTINCT nodes.user, COUNT(*) * 100.0 / (SELECT COUNT(*) FROM nodes)\n",
    "FROM nodes\n",
    "GROUP BY nodes.user\n",
    "ORDER BY (COUNT(*) * 100.0 / (SELECT COUNT(*) FROM nodes)) DESC\n",
    ";\n",
    "'''\n",
    "\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "contributors=pd.DataFrame([all_rows]).T\n",
    "print(type(contributors))\n",
    "##contributors.str.split(pat=\",\")\n",
    "print(contributors)\n",
    "##seaborn.distplot(contributors, kde=True);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are main shops keys and number for each key, list first 40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0\n",
      "0   (clothes, 84)            \n",
      "1   (bakery, 74)             \n",
      "2   (hairdresser, 47)        \n",
      "3   (convenience, 36)        \n",
      "4   (supermarket, 23)        \n",
      "5   (butcher, 22)            \n",
      "6   (optician, 21)           \n",
      "7   (beauty, 18)             \n",
      "8   (shoes, 14)              \n",
      "9   (interior_decoration, 14)\n",
      "10  (books, 14)              \n",
      "11  (car_repair, 12)         \n",
      "12  (bicycle, 12)            \n",
      "13  (florist, 11)            \n",
      "14  (car, 11)                \n",
      "15  (sports, 10)             \n",
      "16  (laundry, 10)            \n",
      "17  (jewelry, 10)            \n",
      "18  (newsagent, 8)           \n",
      "19  (furniture, 8)           \n"
     ]
    }
   ],
   "source": [
    "LIST_NTSHOPS=\"\"\"\n",
    "select value,count(value)\n",
    "from nodes_tags\n",
    "WHERE key ='shop'\n",
    "group by value\n",
    "order by count(value) desc\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "c.execute(LIST_NTSHOPS)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z.iloc[0:20,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       0\n",
      "20  (art, 8)            \n",
      "21  (mobile_phone, 7)   \n",
      "22  (kitchen, 7)        \n",
      "23  (kiosk, 7)          \n",
      "24  (greengrocer, 7)    \n",
      "25  (alcohol, 7)        \n",
      "26  (estate_agent, 6)   \n",
      "27  (cheese, 6)         \n",
      "28  (yes, 5)            \n",
      "29  (tobacco, 5)        \n",
      "30  (deli, 5)           \n",
      "31  (travel_agency, 4)  \n",
      "32  (tattoo, 4)         \n",
      "33  (seafood, 4)        \n",
      "34  (motorcycle, 4)     \n",
      "35  (convenience;gas, 4)\n",
      "36  (confectionery, 4)  \n",
      "37  (bag, 4)            \n",
      "38  (weapons, 3)        \n",
      "39  (pastry, 3)         \n"
     ]
    }
   ],
   "source": [
    "print(z.iloc[20:40,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are other informations for nodes shops.yes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOPSYES=\"\"\"\n",
    "select nodes_tags.id, nodes_tags.key, nodes_tags.value from nodes_tags,\n",
    "((select nodes.id as yesyes\n",
    "from  nodes_tags left join nodes\n",
    "ON nodes.id=nodes_tags.id\n",
    "AND nodes_tags.key ='shop'\n",
    "AND nodes_tags.value ='yes') as sousprog)\n",
    "\n",
    "WHERE nodes_tags.id = yesyes\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(SHOPSYES)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are other informations for nodes shops.cheese ( five shops selling exclusively cheese, we are in French mountains)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOPSCHEESE=\"\"\"\n",
    "select nodes_tags.id, nodes_tags.key, nodes_tags.value from nodes_tags,\n",
    "((select nodes.id as cheeses\n",
    "from  nodes_tags left join nodes\n",
    "ON nodes.id=nodes_tags.id\n",
    "AND nodes_tags.key ='shop'\n",
    "AND nodes_tags.value ='cheese') as sousprog)\n",
    "\n",
    "WHERE nodes_tags.id = cheeses\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(SHOPSCHEESE)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are other informations for nodes shops.deli**  \n",
    "_( Shop focused on selling delicatessen , possibly also fine wine. Not to be confused with the US delis.)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOPSDELI=\"\"\"\n",
    "select nodes_tags.id, nodes_tags.key, nodes_tags.value from nodes_tags,\n",
    "((select nodes.id as delis\n",
    "from  nodes_tags left join nodes\n",
    "ON nodes.id=nodes_tags.id\n",
    "AND nodes_tags.key ='shop'\n",
    "AND nodes_tags.value ='deli') as sousprog)\n",
    "\n",
    "WHERE nodes_tags.id = delis\n",
    "order by nodes_tags.id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(SHOPSDELI)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the main amenities, list first 50 and count for each key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_NTAMEN=\"\"\"\n",
    "select value,count(value)\n",
    "from nodes_tags\n",
    "WHERE key ='amenity'\n",
    "group by value\n",
    "order by count(value) desc\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "c.execute(LIST_NTAMEN)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z.iloc[0:50,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are main \"historic\" keys ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_HISTORIC=\"\"\"\n",
    "select value,count(value)\n",
    "from nodes_tags\n",
    "WHERE key ='historic'\n",
    "group by value\n",
    "order by count(value) desc\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(LIST_HISTORIC)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 5 archeological sites, what are other informations (nodes_tags) about those archeological sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCH_HISTORIC=\"\"\"\n",
    "select nodes_tags.id, nodes_tags.key, nodes_tags.value from nodes_tags,\n",
    "((select nodes.id as sitesarcheo\n",
    "from  nodes_tags left join nodes\n",
    "ON nodes.id=nodes_tags.id\n",
    "AND nodes_tags.key ='historic'\n",
    "AND nodes_tags.value ='archaeological_site') as sousprog)\n",
    "\n",
    "WHERE nodes_tags.id = sitesarcheo\n",
    "order by nodes_tags.id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(ARCH_HISTORIC)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "   ## Some links to references used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General**  \n",
    "- Map feature in openstreetmap https://wiki.openstreetmap.org/wiki/Map_Features (list of nodes and tags to use)\n",
    "- Second main user of OSM in Annecy area has hit headlines (:-)https://www.salzburgresearch.at/blog/alpine-ski-world-championship-meets-openstreetmap-who-are-the-mapping-champions/\n",
    "\n",
    "\n",
    "\n",
    "**Solutions to some python coding problems**  \n",
    "- VERBOSE and Re Module https://www.geeksforgeeks.org/verbose-in-python-regex/\n",
    "- csv write and headers https://discuss.codecademy.com/t/how-does-writeheader-know-the-fields-to-use/463772  \n",
    "- how to get nice display of pandas dataframe (correct width) https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe\n",
    "- tool for count of nodes, ways and relations https://gis.stackexchange.com/questions/281788/openstreetmap-determining-number-of-nodes-ways-and-relations-in-a-pbf-file\n",
    "- fo export from osm file to csv file : https://mygeodata.cloud/converter/osm-to-csv#:~:text=Upload%20your%20OSM%20data%20(widely,will%20be%20exported%20as%20well.\n",
    "\n",
    "**Modify database in sql**  \n",
    "https://www.w3schools.com/sql/sql_update.asp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
