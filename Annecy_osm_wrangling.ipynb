{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Project 2: Wrangle OpenStreetMap Data\n",
    "\n",
    " ## Project : cleaning and sql queries of Annecy open street map data\n",
    "\n",
    "![https://github.com/MariannePERAUD/Udacity_Project_2_Data_wrangling_with_sql/blob/master/Annecy.jpg](Annecy.jpg)\n",
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#Intro\">Introduction</a></li>\n",
    "<li><a href=\"#Set-up\">Initial Set-up</a></li>   \n",
    "<li><a href=\"#assess\">Assess data</a></li>\n",
    "<li><a href=\"#identify\">Identify problems and clean data</a></li>\n",
    "<li><a href=\"#transfercsv\">Write nodes and ways as csv files</a></li>\n",
    "<li><a href=\"#createsql\">Create sql databasis</a></li>\n",
    "<li><a href=\"#checkdb\">Some checks on databasis accuracy and completeness prior to inquiries</a></li>\n",
    "<li><a href=\"#querydb\">Sql queries</a></li>\n",
    "<li><a href=\"#references\">Some links to references used</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Intro'></a>\n",
    "## Introduction\n",
    "\n",
    "Area explored is that of Annecy, in the French Alps, a place that I like and where part of my family leaves.\n",
    "Annecy is near French Alps, not far from Switzerland.\n",
    "\n",
    "\n",
    "\n",
    " minlat=\"45.7996000\" minlon=\"5.9336000\" maxlat=\"45.9323000\" maxlon=\"6.2529000\"\n",
    "\n",
    "I will first check file size and content ; \n",
    "\n",
    "Then identify problems and errors and begin data cleaning to make further analysis on a cleaned dataset;  \n",
    "\n",
    "Transform osm data in csv.files ;  \n",
    "\n",
    "From csv.files, create a databasisin sql with appropriate structure ;\n",
    "\n",
    "Analyse databasis with sql\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Set-up'></a>\n",
    "## Set-up\n",
    "Import necessary python modules for the analysis and define path to dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8import xml.etree.cElementTree as ET \n",
    "import xml.etree.cElementTree as ET \n",
    "import codecs ##to write unicode files\n",
    "import pprint ## to print easier to read dictionnaries\n",
    "import csv ## to read and write csv file\n",
    "import os ## to get file size \n",
    "import pandas as pd ## I am more familiar with dataframe use than plan dictionnaries\n",
    "import re ## to search characters in dataset\n",
    "import sqlite3 ## to use SQL databasis\n",
    "from collections import defaultdict ## to avoid Keyerrors. Will return default value for missing values. \n",
    "                                    ##As advised by Udacity:-)\n",
    "DATASET = \"SmallAnnecy.osm\"\n",
    "\n",
    "pd.set_option('display.width', 2000) ## in order to get nicer display of pandas dataframes on screen\n",
    "pd.set_option('max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "   ## Assess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check that size of the file is greater than 50Mb#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=os.path.getsize(DATASET)/1000000\n",
    "size=round(size)\n",
    "##Map's size and references\n",
    "##Source: https://prograide.com/pregunta/6464/obtenir-la-taille-du-fichier-en-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Count number of different tags in the file#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tags={}\n",
    "def count_tags(filename):\n",
    "    for event, elem in ET.iterparse(filename, events=(\"start\",)):\n",
    "        if elem.tag in tags.keys():\n",
    "            tags[elem.tag] += 1\n",
    "        else:\n",
    "            tags[elem.tag] = 1\n",
    "    \n",
    "    return tags\n",
    "\n",
    "tags = count_tags(DATASET)\n",
    "\n",
    "  \n",
    "    ## see summary at the end of paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Count Number of users as of August 28th 2020 (extraction date)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user(element):\n",
    "    return\n",
    "\n",
    "\n",
    "def process_map(filename):\n",
    "    \"\"\"\n",
    "    Count the user id in the filename.\n",
    "    \"\"\"\n",
    "    users = set()\n",
    "    for Marianne, element in ET.iterparse(filename):\n",
    "        try:\n",
    "            users.add(element.attrib['uid'])\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    return users\n",
    "\n",
    "users = process_map(DATASET)\n",
    "\n",
    "    ##see summary at the end of paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Statistics summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Annecy.osm file is 10 Mb, so higher than 50Mb requested\n",
      "\n",
      "Number of tags per type\n",
      "              0\n",
      "osm       1    \n",
      "note      1    \n",
      "meta      1    \n",
      "bounds    1    \n",
      "node      32582\n",
      "tag       27879\n",
      "way       4613 \n",
      "nd        43133\n",
      "relation  240  \n",
      "member    34437\n",
      "\n",
      "319 users until August 28th 2020\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Size of Annecy.osm file is\",size,\"Mb, so higher than 50Mb requested\")\n",
    "print(\"\")\n",
    "print(\"Number of tags per type\")\n",
    "print(pd.DataFrame([tags]).T)\n",
    "print(\"\")\n",
    "print(len(users),\"users until August 28th 2020\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='identify'></a>\n",
    "   ## Identify and clean problems\n",
    "\n",
    "Analyses focuses on identifying problems in street names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assess various types of street names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "French street names are beginning with streetname type,  \n",
    "Rue Lafayette or Allées Wilson etc...  \n",
    "In another node, you can find housenumber  \n",
    "Decoding of Annecy street names has to be a little different from that of US street names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##FICHAUDIT = \"Annecy.osm\"\n",
    "\n",
    "##identifies name of the street in French addresses at the beginning of the address entry, in the form of a string befor\n",
    "\n",
    "street_type_re = re.compile(r\"\"\"\n",
    "^                                    ## for the beginning of the string\n",
    "\\b                                   ## might be empty, but only at the beginning\n",
    "\\S+                                  ## then followed by any character that is not a space\n",
    "\\.?\"\"\",                              \n",
    "re.IGNORECASE|re.VERBOSE) \n",
    "\n",
    "\n",
    "\n",
    "expected = [\"Routes\",\"Maison\",\"ZA\",\"ZI\",\"Palais\",\"Parc\",\"Angon\",\"Escaliers\",\"Le\",\"Les\",\"Lieudit\",\"Chemin\",\"Esplanade\",\"Faubourg\",\"Passage\",\"Pont\",\"Port\",\"Rue\", \"Boulevard\",\"Place\",\"Allée\",\"Avenue\",\"Impasse\",\"Col\",\"Côte\",\"Quai\",\"Rampe\",\"Route\",\"Promenade\",\"Square\",\"Voie\"]\n",
    "##when iterating on the query, we will see that there are many valid possible entries other than rue (=street)\n",
    "\n",
    "##treet_type_re = re.compile(r'^\\b\\S+\\.?', re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nested functions in order to audit open street map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## create street types set(ie strings at the begining of tags before a blanc)\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "           \n",
    "            street_types[street_type].add(street_name)\n",
    "\n",
    "## define condition : is in street types\n",
    "def is_street_name(elem):\n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "## parse all tags for node and way and list all strings corresponding to street types in osm file\n",
    "def audit(osmfile):\n",
    "    osm_file = open(DATASET, \"r\",encoding='utf-8')\n",
    "    street_types = defaultdict(set)\n",
    "    ##print (street_types)\n",
    "    i=0\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)):\n",
    "        \n",
    "        if elem.tag == \"node\" or elem.tag == \"way\":\n",
    "           \n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "\n",
    "    return street_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fg': {'fg des balmettes'},\n",
      " 'rue': {'rue Royale',\n",
      "         'rue Sommeiller',\n",
      "         'rue de la Gare',\n",
      "         'rue de la Garr',\n",
      "         'rue de la gare',\n",
      "         'rue de la préfecture',\n",
      "         'rue des Glières',\n",
      "         'rue du Président Favre',\n",
      "         'rue du Travail'}}\n"
     ]
    }
   ],
   "source": [
    "##run funtion audit on osm file and print \"unexpected street type dictionnary\"\n",
    "\n",
    "\n",
    "st_types = audit(DATASET)\n",
    "\n",
    "pprint.pprint(dict(st_types))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New mapping dictionary 13 and Georges are not changed at this stage  \n",
    "                         13 should go in another tag (housenumber) and name of the street type should be added to  \n",
    "                         street_nametag at a later stage; \n",
    "Lower case is replaced and abbreviation fg is replaced by Faubourg.\n",
    "\n",
    "If users are not paying attention to street names, can we consider dataset as reliable ?\n",
    "It depends on what we are looking for.\n",
    "Where I leave, I use open street map for recommended itineraries by bicycle, so street names are not so important for me.\n",
    "What is more important is what I can find on my way (shops, parks...).\n",
    "\n",
    "However, let's replace wrong entries in the file that will be imported for queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replace wrong upercase/lower case**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "mapping = { \"13\": \"13\",\n",
    "           \"Georges\": \"Georges\",\n",
    "            \"allée.\": \"Allée\",\n",
    "            \"chemin\": \"Chemin\",\n",
    "            \"fg\": \"Faubourg\",\n",
    "            \"route\": \"Route\",\n",
    "          \"rue\": \"Rue\"}\n",
    "\n",
    "#update name creates a new dictionnary with \"before and after\"street type entries\n",
    "def update_name(name):\n",
    "    street_type= name.split(' ',1)[0]\n",
    "    street_name= name.split(' ',1)[-1]        \n",
    "\n",
    "    if street_type in mapping:\n",
    "        name = mapping[street_type] + ' ' + street_name  \n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run update creates a new dictionnary where street type entries are corrected when necessary\n",
    "def run_updates(filename):\n",
    "    \n",
    "    for st_type, ways in st_types.items():\n",
    "        for name in ways:\n",
    "            better_name = update_name(name)\n",
    "            if better_name != name:\n",
    "                corrected_names[name] = better_name\n",
    "    return corrected_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**QUICK CHECK OF MODIFICATIONS PROPOSAL**                                              0\n",
      "rue des Glières         Rue des Glières       \n",
      "rue de la Gare          Rue de la Gare        \n",
      "rue de la Garr          Rue de la Garr        \n",
      "rue Royale              Rue Royale            \n",
      "rue du Président Favre  Rue du Président Favre\n",
      "rue de la préfecture    Rue de la préfecture  \n",
      "rue de la gare          Rue de la gare        \n",
      "rue Sommeiller          Rue Sommeiller        \n",
      "rue du Travail          Rue du Travail        \n",
      "fg des balmettes        Faubourg des balmettes\n"
     ]
    }
   ],
   "source": [
    "#corrected names creation for osm file\n",
    "corrected_names = {}   \n",
    "corrected_names = run_updates(DATASET)\n",
    "check2=(pd.DataFrame([corrected_names]).T) \n",
    "print(\"**QUICK CHECK OF MODIFICATIONS PROPOSAL**\",check2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='transfercsv'></a>\n",
    "   ## Write nodes and ways as csv files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File is now ready for import in a csv file and then in SQL databasis.\n",
    "We will create five csv files preparing five SQL tables creation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# Make sure the fields order in the csvs matches the column order in the sql table schema\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position']\n",
    "\n",
    "# Prepare modification of dictionnary to import with modified street names\n",
    "def correct_element(v):\n",
    "    if v in corrected_names:\n",
    "        correct_value = corrected_names[v]\n",
    "    else:\n",
    "        correct_value = v\n",
    "    return correct_value\n",
    "\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                   default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "\n",
    "    if element.tag == 'node':\n",
    "        node_attribs['id'] = element.attrib['id']\n",
    "        node_attribs['user'] = element.attrib['user']\n",
    "        node_attribs['uid'] = element.attrib['uid']\n",
    "        node_attribs['version'] = element.attrib['version']\n",
    "        node_attribs['lat'] = element.attrib['lat']\n",
    "        node_attribs['lon'] = element.attrib['lon']\n",
    "        node_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        node_attribs['changeset'] = element.attrib['changeset']\n",
    "        \n",
    "        for node in element:\n",
    "            tag_dict = {}\n",
    "            tag_dict['id'] = element.attrib['id']\n",
    "            if ':' in node.attrib['k']:\n",
    "                tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            else:\n",
    "                tag_dict['type'] = 'regular'\n",
    "                tag_dict['key'] = node.attrib['k']\n",
    "                tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "            tags.append(tag_dict)\n",
    "            \n",
    "    elif element.tag == 'way':\n",
    "        way_attribs['id'] = element.attrib['id']\n",
    "        way_attribs['user'] = element.attrib['user']\n",
    "        way_attribs['uid'] = element.attrib['uid']\n",
    "        way_attribs['version'] = element.attrib['version']\n",
    "        way_attribs['timestamp'] = element.attrib['timestamp']\n",
    "        way_attribs['changeset'] = element.attrib['changeset']\n",
    "        n = 0\n",
    "        for node in element:\n",
    "            if node.tag == 'nd':\n",
    "                way_dict = {}\n",
    "                way_dict['id'] = element.attrib['id']\n",
    "                way_dict['node_id'] = node.attrib['ref']\n",
    "                way_dict['position'] = n\n",
    "                n += 1\n",
    "                way_nodes.append(way_dict)\n",
    "            if node.tag == 'tag':\n",
    "                tag_dict = {}\n",
    "                tag_dict['id'] = element.attrib['id']\n",
    "                if ':' in node.attrib['k']:\n",
    "                    tag_dict['type'] = node.attrib['k'].split(':', 1)[0]\n",
    "                    tag_dict['key'] = node.attrib['k'].split(':', 1)[-1]\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                else:\n",
    "                    tag_dict['type'] = 'regular'\n",
    "                    tag_dict['key'] = node.attrib['k']\n",
    "                    tag_dict['value'] = correct_element(node.attrib['v'])\n",
    "                tags.append(tag_dict)\n",
    "    \n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    \"\"\"Yield element if it is the right type of tag\"\"\"\n",
    "\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "\n",
    "\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    \"\"\"Extend csv.DictWriter to handle Unicode input\"\"\"\n",
    "\n",
    "    def writerow(self, row):\n",
    "        super(UnicodeDictWriter, self).writerow({\n",
    "            k: (v.encode('utf-8') if isinstance(v, str) else v) for k, v in row.items()\n",
    "        })\n",
    "\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "def process_map(file_in):\n",
    "    \"\"\"Iteratively process each XML element and write to csv(s)\"\"\"\n",
    "\n",
    "    ## create csv files\n",
    "    with codecs.open(NODES_PATH, 'w',encoding='utf-8') as nodes_file, \\\n",
    "    codecs.open(NODE_TAGS_PATH, 'w',encoding='utf-8') as nodes_tags_file, \\\n",
    "    codecs.open(WAYS_PATH, 'w',encoding='utf-8') as ways_file, \\\n",
    "    codecs.open(WAY_NODES_PATH, 'w',encoding='utf-8') as way_nodes_file, \\\n",
    "    codecs.open(WAY_TAGS_PATH, 'w',encoding='utf-8') as way_tags_file:\n",
    "\n",
    "        nodes_writer = csv.DictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = csv.DictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = csv.DictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = csv.DictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = csv.DictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "    ## create headers in csv files\n",
    "        nodes_writer.writeheader()\n",
    "        node_tags_writer.writeheader()\n",
    "        ways_writer.writeheader()\n",
    "        way_nodes_writer.writeheader()\n",
    "        way_tags_writer.writeheader()\n",
    "\n",
    "        \n",
    "\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if element.tag == 'node':\n",
    "                nodes_writer.writerow(el['node'])\n",
    "                node_tags_writer.writerows(el['node_tags'])\n",
    "            elif element.tag == 'way':\n",
    "                ways_writer.writerow((el['way']))\n",
    "                way_nodes_writer.writerows(el['way_nodes'])\n",
    "                way_tags_writer.writerows(el['way_tags'])\n",
    "                    \n",
    "process_map(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='createsql'></a>\n",
    "   ## Create sql databasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that .csv are created, create tables that will shelter sql databasis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Creating database on disk\n",
    "sqlite_file = 'Annecy.db'\n",
    "conn = sqlite3.connect('Annecy.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''DROP TABLE IF EXISTS nodes''')\n",
    "c.execute('''DROP TABLE IF EXISTS nodes_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_tags''')\n",
    "c.execute('''DROP TABLE IF EXISTS ways_nodes''')\n",
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create sql tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_NODES = \"\"\"\n",
    "\n",
    "\n",
    "CREATE TABLE nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    lat REAL,\n",
    "    lon REAL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_NODES_TAGS = \"\"\"\n",
    "CREATE TABLE nodes_tags (\n",
    "    id INTEGER,\n",
    "    key TEXT,\n",
    "    value TEXT,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS = \"\"\"\n",
    "CREATE TABLE ways (\n",
    "    id INTEGER NOT NULL,\n",
    "    user TEXT,\n",
    "    uid INTEGER,\n",
    "    version INTEGER,\n",
    "    changeset INTEGER,\n",
    "    timestamp TEXT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_TAGS = \"\"\"\n",
    "CREATE TABLE ways_tags (\n",
    "    id INTEGER NOT NULL,\n",
    "    key TEXT NOT NULL,\n",
    "    value TEXT NOT NULL,\n",
    "    type TEXT,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "QUERY_WAYS_NODES = \"\"\"\n",
    "CREATE TABLE ways_nodes (\n",
    "    id INTEGER NOT NULL,\n",
    "    node_id INTEGER NOT NULL,\n",
    "    position INTEGER NOT NULL,\n",
    "    FOREIGN KEY (id) REFERENCES ways(id),\n",
    "    FOREIGN KEY (node_id) REFERENCES nodes(id)\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "c.execute(QUERY_NODES)\n",
    "c.execute(QUERY_NODES_TAGS)\n",
    "c.execute(QUERY_WAYS)\n",
    "c.execute(QUERY_WAYS_TAGS)\n",
    "c.execute(QUERY_WAYS_NODES)\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodes.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db1 = [(i['id'], i['lat'], i['lon'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('nodes_tags.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db2 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db3 = [(i['id'], i['user'], i['uid'], i['version'], i['changeset'], i['timestamp']) for i in dr]\n",
    "    \n",
    "with open('ways_tags.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db4 = [(i['id'], i['key'], i['value'], i['type']) for i in dr]\n",
    "    \n",
    "with open('ways_nodes.csv','rt',encoding='utf8') as fin:\n",
    "    dr = csv.DictReader(fin) # comma is default delimiter\n",
    "    to_db5 = [(i['id'], i['node_id'], i['position']) for i in dr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "c.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?, ?);\", to_db1)\n",
    "c.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db2)\n",
    "c.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db3)\n",
    "c.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db4)\n",
    "c.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db5)\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='checkdb'></a>\n",
    "   ## Some checks on databasis accuracy and completeness prior to inquiries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several preliminary checks on SQL databasis\n",
    "- Check that databasis is complete (same number of nodes and ways as computed before)\n",
    "- Check that some wrong street names have been corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32582,)]\n",
      "[(4613,)]\n"
     ]
    }
   ],
   "source": [
    "c.execute('SELECT COUNT(*) FROM nodes')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)\n",
    "\n",
    "c.execute('SELECT COUNT(*) FROM ways')\n",
    "all_rows = c.fetchall()\n",
    "print(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "CHECK_CHANGES2= \"\"\"\n",
    "SELECT value \n",
    "FROM ways_tags\n",
    "WHERE value ='Chemin de Bellevue'\n",
    "OR value ='Rue Cassiopée'\n",
    "OR value ='rue Cassiopée'\n",
    "OR value ='Chemin de Bellevue'\n",
    "limit 10;\n",
    "\"\"\"\n",
    "c.execute(CHECK_CHANGES2)\n",
    "all_rows=c.fetchall()\n",
    "print(all_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "CHECK_CHANGES3= \"\"\"\n",
    "SELECT value \n",
    "FROM nodes_tags\n",
    "WHERE value ='Chemin de Bellevue'\n",
    "OR value ='Rue Cassiopée'\n",
    "OR value ='rue Cassiopée'\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "c.execute(CHECK_CHANGES3)\n",
    "all_rows=c.fetchall()\n",
    "print(all_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='querydb'></a>\n",
    "   ## SQL queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Enquiries about content of the database in SQL, at least..._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Count number of most active users**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   users, number of contributions\n",
      "0   (botdidier2020, 6778)        \n",
      "1   (Emmanuel Pacaud, 6477)      \n",
      "2   (Virgile1994, 5640)          \n",
      "3   (InfosReseaux, 2615)         \n",
      "4   (chimel38, 2123)             \n",
      "5   (Zedh, 1653)                 \n",
      "6   (Marc Mongenet, 1197)        \n",
      "7   (didier2020, 563)            \n",
      "8   (Musculus, 393)              \n",
      "9   (pyrog, 370)                 \n",
      "10  (dom74, 328)                 \n",
      "11  (Fabien98, 294)              \n",
      "12  (Mickael42, 269)             \n",
      "13  (Verdy_p, 261)               \n",
      "14  (cquest, 228)                \n"
     ]
    }
   ],
   "source": [
    "# Query to show the nicknames *user* and contributions of the top 15 contributors\n",
    "QUERY = '''\n",
    "SELECT DISTINCT nodes.user as USER, COUNT(*) as contribution_number\n",
    "FROM nodes\n",
    "GROUP BY nodes.user\n",
    "ORDER BY COUNT(*) DESC\n",
    "LIMIT 15;\n",
    "'''\n",
    "c.execute(QUERY)\n",
    "all_rows = c.fetchall()\n",
    "contributors=pd.DataFrame([all_rows]).T\n",
    "contributors.columns=([\"users, number of contributions\"])\n",
    "print(contributors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are main shops keys and number for each key, list first 40**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            0\n",
      "0   (clothes, 82)            \n",
      "1   (hairdresser, 19)        \n",
      "2   (bakery, 18)             \n",
      "3   (interior_decoration, 15)\n",
      "4   (shoes, 13)              \n",
      "5   (optician, 13)           \n",
      "6   (books, 10)              \n",
      "7   (beauty, 10)             \n",
      "8   (furniture, 8)           \n",
      "9   (convenience, 8)         \n",
      "10  (art, 8)                 \n",
      "11  (jewelry, 7)             \n",
      "12  (supermarket, 6)         \n",
      "13  (sports, 6)              \n",
      "14  (mobile_phone, 6)        \n",
      "15  (estate_agent, 4)        \n",
      "16  (deli, 4)                \n",
      "17  (butcher, 4)             \n",
      "18  (bicycle, 4)             \n",
      "19  (bag, 4)                 \n"
     ]
    }
   ],
   "source": [
    "LIST_NTSHOPS=\"\"\"\n",
    "select value,count(value)\n",
    "from nodes_tags\n",
    "WHERE key ='shop'\n",
    "group by value\n",
    "order by count(value) desc\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "c.execute(LIST_NTSHOPS)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z.iloc[0:20,])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          0\n",
      "20  (weapons, 3)           \n",
      "21  (travel_agency, 3)     \n",
      "22  (tobacco, 3)           \n",
      "23  (tattoo, 3)            \n",
      "24  (musical_instrument, 3)\n",
      "25  (laundry, 3)           \n",
      "26  (kitchen, 3)           \n",
      "27  (gift, 3)              \n",
      "28  (frame, 3)             \n",
      "29  (confectionery, 3)     \n",
      "30  (beverages, 3)         \n",
      "31  (alcohol, 3)           \n",
      "32  (toys, 2)              \n",
      "33  (ticket, 2)            \n",
      "34  (sewing, 2)            \n",
      "35  (pastry, 2)            \n",
      "36  (outdoor, 2)           \n",
      "37  (music, 2)             \n",
      "38  (houseware, 2)         \n",
      "39  (household_linen, 2)   \n"
     ]
    }
   ],
   "source": [
    "print(z.iloc[20:40,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the main amenities, list first 50 and count for each key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          0\n",
      "0   (restaurant, 118)      \n",
      "1   (bicycle_parking, 110) \n",
      "2   (waste_basket, 87)     \n",
      "3   (bench, 74)            \n",
      "4   (shelter, 29)          \n",
      "5   (fast_food, 29)        \n",
      "6   (cafe, 27)             \n",
      "7   (drinking_water, 26)   \n",
      "8   (vending_machine, 23)  \n",
      "9   (recycling, 21)        \n",
      "10  (parking_entrance, 17) \n",
      "11  (post_box, 14)         \n",
      "12  (bar, 14)              \n",
      "13  (pharmacy, 13)         \n",
      "14  (toilets, 12)          \n",
      "15  (bank, 12)             \n",
      "16  (atm, 11)              \n",
      "17  (charging_station, 10) \n",
      "18  (ferry_terminal, 9)    \n",
      "19  (pub, 8)               \n",
      "20  (school, 7)            \n",
      "21  (fountain, 6)          \n",
      "22  (car_rental, 6)        \n",
      "23  (ice_cream, 5)         \n",
      "24  (bicycle_rental, 4)    \n",
      "25  (theatre, 3)           \n",
      "26  (social_facility, 3)   \n",
      "27  (place_of_worship, 3)  \n",
      "28  (parking_space, 3)     \n",
      "29  (parking, 3)           \n",
      "30  (nightclub, 3)         \n",
      "31  (library, 3)           \n",
      "32  (kindergarten, 3)      \n",
      "33  (driving_school, 3)    \n",
      "34  (taxi, 2)              \n",
      "35  (public_bookcase, 2)   \n",
      "36  (post_office, 2)       \n",
      "37  (motorcycle_parking, 2)\n",
      "38  (community_centre, 2)  \n",
      "39  (clock, 2)             \n",
      "40  (cinema, 2)            \n",
      "41  (car_sharing, 2)       \n",
      "42  (boat_rental, 2)       \n",
      "43  (telephone, 1)         \n",
      "44  (social_centre, 1)     \n",
      "45  (police, 1)            \n",
      "46  (piano, 1)             \n",
      "47  (photo_booth, 1)       \n",
      "48  (music_school, 1)      \n",
      "49  (language_school, 1)   \n"
     ]
    }
   ],
   "source": [
    "LIST_NTAMEN=\"\"\"\n",
    "select value,count(value)\n",
    "from nodes_tags\n",
    "WHERE key ='amenity'\n",
    "group by value\n",
    "order by count(value) desc\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "c.execute(LIST_NTAMEN)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z.iloc[0:50,])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are main \"historic\" keys ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "0  (memorial, 13)\n",
      "1  (yes, 1)      \n",
      "2  (ruins, 1)    \n"
     ]
    }
   ],
   "source": [
    "LIST_HISTORIC=\"\"\"\n",
    "select value,count(value)\n",
    "from nodes_tags\n",
    "WHERE key ='historic'\n",
    "group by value\n",
    "order by count(value) desc\n",
    "\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(LIST_HISTORIC)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There are 5 archeological sites, what are other informations (nodes_tags) about those archeological sites**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [0]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "ARCH_HISTORIC=\"\"\"\n",
    "select nodes_tags.id, nodes_tags.key, nodes_tags.value from nodes_tags,\n",
    "((select nodes.id as sitesarcheo\n",
    "from  nodes_tags left join nodes\n",
    "ON nodes.id=nodes_tags.id\n",
    "AND nodes_tags.key ='historic'\n",
    "AND nodes_tags.value ='archaeological_site') as sousprog)\n",
    "\n",
    "WHERE nodes_tags.id = sitesarcheo\n",
    "order by nodes_tags.id\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "c.execute(ARCH_HISTORIC)\n",
    "all_rows=c.fetchall()\n",
    "z=pd.DataFrame([all_rows]).T\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='references'></a>\n",
    "   ## Some links to references used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Map feature in openstreetmap https://wiki.openstreetmap.org/wiki/Map_Features (list of nodes and tags to use)\n",
    "\n",
    "Solutions to some python coding problems\n",
    "- VERBOSE and Re Module https://www.geeksforgeeks.org/verbose-in-python-regex/\n",
    "- csv write and headers https://discuss.codecademy.com/t/how-does-writeheader-know-the-fields-to-use/463772  \n",
    "- how to get nice display of pandas dataframe (correct width) https://stackoverflow.com/questions/11707586/how-do-i-expand-the-output-display-to-see-more-columns-of-a-pandas-dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
